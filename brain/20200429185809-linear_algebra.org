#+TITLE: Linear Algebra
- tags :: [[file:20200424162958-algebra.org][Algebra]]

* Definitions
** <<<Matrix>>>
A "rectangular" array of scalar values
** <<<Vector>>>
A "linear" array of scalar values
They can be row vectors or column vectors
*** Row Vector
$\vec{x}=[x_1\, x_2\, \dots\, x_N]$
*** Column Vector
$\vec{x}=\left[\begin{matrix}x_1\\ x_2\\ \vdots\\ x_N\end{matrix}\right]$
* Types of Matrices
** <<<Square>>>
$A:\mathbb R^{n\times n},\, n\in\mathbb Z^{+}_{\varnothing}$
** Vandermonde
  $V=\left[\begin{matrix}1 &\alpha_1&\alpha_1^{2}&\dots&\alpha_1^{n-1}\\ 1& \alpha_2&\alpha_2^{2}&\dots&\alpha_2^{n-1}\\\vdots&\vdots&\vdots&\ddots&\vdots\\1&\alpha_m&\alpha_m^2&\dots&\alpha_m^{n-1}\end{matrix} \right]$
** Hollow
Square Matrix wich diagonal is zero

** <<<Sparse>>> Matrix/Vector
- most elements are zero
*** r-sparse
It has r elements different from zero

* Operations
** Matrices
*** Matrix product
given 2 matrices $A:\mathbb{R}^{m\times n}$, $B:\mathbb{R}^{n\times p}$, their product
 $AB=C:\mathbb{R}^{m\times p}$ is determined by its elements ${(i,j)}$:
 $$C_{i,j}=\sum\limits_{k=1}^{n}A_{i,k}B_{k,j}$$

*** <<<Induced Norm>>>
Given a vector $\vec{x}:\mathbb R^{n}$ and a norm $\|\cdot\|$ defined for $\mathbb R^{n}$, the induced norm $\|A\|$ for a matrix $A:\mathbb R^{n\times n}$ is:
- $$\|A\|=\mathrm{max}\{\|A\vec{x}\|:\vec x\in\mathbb R^{n},\|\vec x\|=1\}$$
- $$\|A\|=\mathrm{sup}\{\frac{\|A\vec{x}\|}{\|\vec{x}\|}:\vec x\in\mathbb R^{n},\vec{x}\neq0\}$$

*** Hadamard product
- or Schur product

$(A\circ B)_{ij}=(a_{ij}b_{ij})$
Element-wise multiplication =.*=

*** Hadamard root
*** Hadamard inverse
$A^{\circ(-1)}= ({1\over a_{ij}})$

*** Hadamard exponential
$e^{\circ A}= (e^{a_{ij}})$

** Vectors
*** <<<Norm>>>
Given $\vec{x}:\mathbb R^{n}$ and $\vec{y}:\mathbb R^{n}$, and a absolute function $|\cdot|$ for scalars.
A norm $\|\cdot\|$ defined for $\mathbb{R}^{n}$ is such:
- $\|\vec{x}+\vec{y}\|\leq\|\vec{x}\|+\|\vec{y}\|$
- $\|\alpha\vec{x}\|=|\alpha|\|\vec{x}\|$
- $\|\vec{x}\|=0 \leftrightarrow \vec{x}=0$

  Usually norms are used to compute distances, since they are non-negative. See [[file:20210413114122-voronoi_diagram.org][Voronoi diagram]]

* Identities
- $(\Gamma H \Gamma^T)^T=\Gamma H \Gamma^T$ if $H^T=H$
- $(AB)^T = B^TA^T$
- $(AB)^{-1Â } = B^{-1}A^{-1}$

* Properties
** Matrix
*** <<<Eigenvalue>>>
Given a $n$ square matrix $A$, its eigenvalues $\lambda:\mathbb C$ associated with the eigenvectors $\vec v$ are such $$A\vec v = \lambda\vec v$$
*** <<<Eigenvector>>>
Given a $n$ square matrix $A$, its eigenvectors $\vec v:\mathbb R^{n},\vec x^T\vec x\neq0$ associated with the eigenvalues $\lambda$ are such $$A\vec v = \lambda\vec v$$

*** Spectral radius
Given a n square matrix $A$, and its eigenvalues $\lambda$, its spectral radius is $$\rho(A)=\mathrm{max}\{|\lambda_i|:\lambda_i \in \lambda\}$$

*** Conditioning
Given $A:\mathbb R^{n\times n}$ invertible, a norm $\|\cdot\|$ defined for $\mathbb R^{n}$ and a induced norm $\|\cdot\|$ for $\mathbb R^{n\times n}$. The conditioning of a matrix is given by: $$\kappa(A)=\|A\|\|A^{-1}\|$$
** Induced Norm
- $\|Ax\|\leq\|A\|\|x\|$
- $\|A+B\|\leq\|A\|+\|B\|$
- $\|AB\|\leq\|A\|\|B\|$
- $\|\alpha A\|=|\alpha|\|A\|$
