#+title: Spring School on Data-driven Model Learning of Dynamic Systems
#+OPTIONS: toc:nil -:nil ^:nil
#+roam_key: https://spring-id-2021.sciencesconf.org/
#+ROAM_TAGS: courses

- tags :: [[file:formations_these.org][Formations Doctorat]]


* System Identification: data-based modeling
<2021-04-06 mar. 14:00-15:00>
- lecturer :: [[file:xavier_bombois.org][Xavier Bombois]]

** Dynamical systems
** Data-based modeling
*** DC motor
- Input :: Applied voltage [V]
- Output ::  measured rotation speed [rad/s]
**** Collect data
**** Model
- Model :: Discrete Time Transfer Function
  Collected data has been created by "computer"
  - ZOH + Continuous System + Sampling
- Motor is linear?? Not??
- Find model that reduces $\epsilon(t)=y(t)-\hat y(t)$
*** Why it is useful?
- First-principle modeling = modeling using the laws of physics
**** Example 1: Track Tracking in a Cd Player
- Arm is supposed rigid -> model is a second order integrator
- first-principle doesn't work sometimes (error on hypothesis)
**** [#A] Example 2: Signal equalization in mobile telephony
- First-principle is almost impossible (mobile moves, different environments)
- Received signal is made up from several delayed versions of the input signal (reflexions and refractions)
- [[https://en.wikipedia.org/wiki/Wiener_filter][Wiener filter]]
***** Use of a known sequence
- Training sequence is sent before the signal of interest, so channel can be estimated ( mobile can be moving )
- FIR order 6
*** Players in system [[file:20210323094314-identification.org][Identification]]
- Input $u(t)$ (freely chosen)
- Ouput $y(t)$ (can be measured)
  + Contribution due to $u(t)$ :: $u(t)$ $G_Ou(t)$
  + Contribution independent of $u(t)$ :: disturbance $v(t)$
*** Identification procedure
pdf:~/these/formations/Data-driven/MATERIAL_TO_BE_SENT/SLIDES_THEORY_APRIL_6_7/slides_spring_1.pdf::25
- Prior Knowledge
- Validate model
  + Construct Model
    - Experiment design
      + Data
    - Model Set
    - Identification criterion
* Prediction Error Identification
<2021-04-06 mar. 15:19-16:35>
** Introduction about Prediction Error Identification
*** Assumptions
**** System TF $G_0(z)$ and Error TF $H_0(z)$
- $H_0$ is stable, inversibly stable and [[file:20200504164021-control.org::*Monic Transfer function][monic]]
- $e(t)$ is a white noite
  + $E[e(t)]=0$
  + $R_e(\tau)\stackrel{\triangle}{= }E[e(t)e(t-\tau)]=\sigma^2\cdot\delta(\tau)$
** Objective
- find best parametric models $g(z,\theta)$ and $H(z,\theta)$
** Predictor $\hat y(t,\theta)$ in identification and prediction error $\epsilon(t,\theta)$
$\epsilon(t,\theta)\stackrel{\triangle}{= }H(z-\theta)^{-1} (y(t)-G(z,\theta)u(t))\,\,\forall t=1..N$
** Properties of the prediction error $\epsilon(t,\theta)$
1. given $\theta$ and $Z^N$, then $\epsilon(t,\theta)$ computable
2. $\epsilon(t,\theta_0)=e(t)$
3. $\epsilon(t,\theta_0)\neq$ white noise for all $\theta\neq\theta_0$
4. $\theta_0$ minimizes power $\bar E[\epsilon^2(t,\theta)]$ of $\epsilon(\theta)$
   a. $\bar E[\epsilon^2(t,\theta)\stackrel{\triangle}{= }\lim_{n\to\infty}{1\over N}\sum_ {t=1}^{N}\epsilon^2(t,\theta)$

** Mathematical Criterion for prediction error identification
*** Optimization problem
- Cost function :: $\hat V(\theta)=\hat E[\epsilon^2(t,\theta)]=\lim_{N\to\infty}{1\over N}\sum_{t=1}^NE[\epsilon^2(t,\theta)]$
- Criterion is impossible since N cannot be $\infty$
*** Tractable Identification Criterion
- Cost function :: $V_{N}(\theta,Z^N)={1\over N}\sum_{t=1}^N\epsilon^2(t,\theta)$
- Estimation through minimization of $V_N$ :: $\hat \theta_N=\mathrm{arg}\, \underset{\theta}{\min}\, V_N(\theta,Z^N)$
** Black box model structures
*** General parametrization used in Matlab Toolbox
pdf:~/these/formations/Data-driven/MATERIAL_TO_BE_SENT/SLIDES_THEORY_APRIL_6_7/slides_spring_2.pdf::32
- $G(z,\theta)={z^{-n_k}B(z,\theta)\over F(z,\theta)A(z,\theta)}$ and $H(z,\theta)={C(z,\theta)\over D(z,\theta)A(z,\theta)}$
- common dynamics use $A(z,\theta)$
- Usually $A(z,\theta)$ is 1
*** Model Structures used in practice
- ARX - $n_k$, $A$ and $B$
- ARMAX $n_k$ $B$ $A$ $C$
- OE - Output Error $n_k$ $B$ $F$
- FIR $n_k$ $B$
- BJ - Box-Jenkins (can represent anything) $B$ $C$ $F$ $D$ $n_k$ <= recommended

** Computation
- if $y(t,\theta)$ is linear w.r.t. $\theta$ FIR or ARX


- Least Squares :: $\hat \theta = \left[{1 \over N} \sum_{t=1}^N\phi(t)\phi^T(t)\right]^{-1}\cdot\left[{1 \over N} \sum_{t=1}^N\phi(t)y(t)\right]$
** Conditions on experimental data
<2021-04-06 mar. 16:55-17:55>
#+begin_quote
has a unique solution θ ∗ (i.e. θ ∗ = θ 0 when S ∈ M) if the
input signal u(t) that is chosen to generate the experimental
data is sufficiently rich.
#+end_quote
** Revision
<2021-04-07 mer. 08:00-08:15>
** Statistical distribution of the identified model
<2021-04-07 mer. 08:15-08:49>
$cov(G(e^{j\omega,\hat\theta_N}))=E[|G(e^{j\omega},\hat\theta_N)-G(e^{j\omega},\theta_0)|^2]\approx\Lambda_{G}(e^{j\omega},\theta_0)(E[(\hat\theta_N-\theta_0)(\hat\theta_N-\theta_0)^T])\Lambda^\star_G$
- The larger N and/or the larger the power of u(t), the smaller $cov(G(e j\omega , θ̂ N ))$
** Validation
use standard deviation $\sqrt{cov(G(e^{j\omega},\hat\theta_N))}$
- $|G(e^{j\omega},\hat\theta_N)-G(e^{j\omega},\theta_0)|<2.45\sqrt{cov(G(e^{j\omega},\hat\theta_N))}$ w.p. 95%
  + 2.45 is the $2\sigma$ confidence intervals for complex-valued normal distribution
- using Nyquist plot each point of the nominal Nyquist plot has a circle with radius $2.45\sigma$
** What is a small $\sqrt{\sigma}$?
- rule of thumb :: $\sqrt{cov(G(e^{j\omega},\hat\theta_N)}<0.1|G(e^{j\omega},\hat\theta_N)|$ in the bandwidth
** What if it appears too large?
- new identification experiment has to be achieved
  - Options:
    + increase power of $u$
    + increase $N$
    + excite frequencies where the variance is not good
** A special case of undermodeling
<2021-04-07 mer. 09:25-10:38>
- $\mathcal{S}\notin \mathcal{M}$
- $\theta_0$ doesn't exist but still there is a $\theta^\star$
- Important case $\mathcal{S}\notin\mathcal{M}$, $G_0\in\mathcal{G}$
  - order of $G_0$ can come from insight
  - $H_0$ is bit more difficult
** What can be said about $\theta^\star$
- 2 cases:
  - $\mathcal{M}$ no common parameters in $G(\theta)$ and $H(\theta)$ (ex: OE, BJ, FIR)
  - $\mathcal{M}$ with common parameters in $G(\theta)$ and $H(\theta)$ (ex: ARX, ARMAX)
- If we have no common parameters \to $G(z,\eta^\star)=G(z,\eta_0)=G_0(z)$
** Choice and validation of model order and structure
- How can we verify assumptions? *
  + model structure validation
*** Model structure validation: «a posteriori» verification
- based on $\hat\theta_N$ and $Z^N$, determine if the chosen model structure is:
  + $\mathcal{S}\in\mathcal{M}$
  + $\mathcal{S}\notin\mathcal{M}$ with $G_0\in\mathcal{G}$
  + $\mathcal{S}\notin\mathcal{M}$ with $G_0\notin\mathcal{G}$
*** Model structure validation: Asymptotic case ($N\to\infty$)
- Identified parameter is $\theta^\star$
- Validation considering $R_\epsilon(\tau)$ and $R_{\epsilon u}(\tau)$ of $\epsilon(t,\theta^\star)$
**** Situation A
 $R_\epsilon(\tau)=\sigma_e^2\delta(\tau)$ and $R_{\epsilon u}(\tau)=0\,\forall\tau$
- $\epsilon(t,\theta^\star)=0\cdot u(t)+e(t)\leftrightarrow G(\theta^\star)=G_0$ and $H(\theta^\star)=H_0\leftrightarrow \mathcal{S}\in\mathcal{M}$
**** Situation B
 $R_\epsilon(\tau)\neq\sigma_e^2\delta(\tau)$ and $R_{\epsilon u}(\tau)=0\,\forall\tau$
- $\epsilon(t,\theta^\star)=0\cdot u(t)+{H_0\over H(\theta^\star)}e(t)\leftrightarrow G(\theta^\star)=G_0$ and $H(\theta^\star)\neq H_0\leftrightarrow \mathcal{S}\notin\mathcal{M}$ with $G_0\in\mathcal{G}$ for $\mathcal{M}$ OE, BJ or FIR
**** Situation C
 $R_\epsilon(\tau)\neq\sigma_e^2\delta(\tau)$ and $\exists \tau s.t.\,R_{\epsilon u}(\tau)\neq0$
- or $\mathcal{S}\neq\mathcal M with G_0\in\mathcal G$ for $\mathcal M$ ARX or ARMAX
- or $\mathcal{S}\neq\mathcal M with G_0\notin\mathcal G$
*** Model structure validation: Pratical case ($N<\infty$)
- when $N$ is finite, even if $\mathcal S\in\mathcal M$:
  + $\hat R_\epsilon^N(\tau)$ will not be $0\, \forall \tau\neq 0$
  + $\hat R_{\epsilon u}^N(\tau)$ will not be $0\, \forall \tau$
- *BUT* they will be small.
  - In general smaller than when $\mathcal S\neq\mathcal M$
- We can use distribution using 99%-confidence region
| Case                                                    | $\hat R_{\epsilon}^N(\tau)$          | $\hat R_{\epsilon u}^N(\tau)$    |
| $\mathcal{S}\in\mathcal{M}$                               | $\in$ confidence bound       | $\in$ confidence bound   |
| $\mathcal{S}\notin\mathcal{M}$ with $G_0\in\mathcal{G}$ | $\notin$ confidence bound  | $\in$ confidence bound |
| $\mathcal{S}\notin\mathcal{M}$ with $G_0\notin\mathcal{G}$        | $\notin$ confidence bound       | $\notin$ confidence bound   |
*** Example
- [[https://fr.mathworks.com/help/ident/ref/resid.html?searchHighlight=resid&s_tid=doc_srchtitle][resid]] function in matlab
** Typical procedure to identify
<2021-04-07 mer. 09:25-10:38>
1. Choose $u(t)$ and collect $Z^N$
2. Choose model of $\mathcal M$
3. Identification of the $G(z,\hat\theta_N)$ and $H(z,\hat\theta_N)$
4. Is $\mathcal S \in \mathcal M$?
   a. Yes ? Go to 5
   b. No ? Go to 2 and choose another model for $\mathcal M$
5. Is $\sqrt{cov(G(e^{j\omega},\hat\theta_N))}$ ($\sqrt{cov(H(e^{j\omega},\hat\theta_N))}$) small?
   b. Yes? Stop
   a. No? Go back to 1

* Frequency-domain Identification (ETFE)
** General objective
** Empirical Transfer Function Estimate (ETFE)
- Time-Domain data \to Frequency-Domain data via (scaled) Fourier Transform
*** Practical aspects
  - Information contained from $t=0\dots(N-1)$ is contained at the $N\over2$ frequencies $\omega_k={2\pi\over N}k$ k=0,1... located in $[0,\pi]$
  - if using multi-sine you only have the signal for those frequencies
** Statistical properties of the ETFE
- stochastic noise $v(t)$ \to ETFE is different at each experiment
*** Variance of the ETFE
$cov(\hat G(e^{j\omega k}))$ \to $\Phi_v(\omega)\over\Phi_u(\omega)$
- with white noise estimate is worse
  + unlike for a multisine, the variance is not proportional to $1\over N$, variance only proportional to $1\over\sigma^2_u$
** Smoothing of ETFE through the use of windows
- ex: use [[https://fr.mathworks.com/help/signal/ref/hamming.html][Hamming window]] $W_\gamma$
*** How to choose $\gamma$?
- SPA
  - since ($G_0(z)$ stable) variance of $\hat R_{yu}(\tau)$ is the same for all $\tau$,  removing greater values $\tau$
* Pratical issues when designing the identification experiment
** Preparatory experiments
- noise measurement on the output
- step response analysis
  + area of linearity
  + time constants
  + static gain
  + delay of the system
** Choice of the sampling frequency $\omega_s={2\pi\over T_s}$
- High $\omega_s$ induces numerical problems
  + T_s should not be too small w.r.t. time response of the system
    - Rule of thumb \to 1 decade after
  + $A_d=e^{A_{cont}T_s}\to I$ when $T_s\to0$
* System Identification Toolbox MATLAB
<2021-04-07 mer. 14:00-14:25>
- present
* Computer Session 1 - 7 avril
** Part 1
*** Q1
- using OE model structure we have $n_k$ $B$ $F$ to identify
  + since $n_b=2$ and $n_f=4$ we have $n_G=6$ and using PEI u has to be persistently excited of order at least $n_G$
*** Q2
Since full order estimation is used; $\mathcal S \in \mathcal M$ and then $\theta^\star=\theta_0$
*** Q3
Yes, they do, both autocorrelation and cross correlation stands inside the confidence bounds.
*** Q4
Since it is ARX, it is supposed that G and H have the same dynamics, so no.
*** Q5
Yes they do not lie in the confidence bounds, $G\notin\mathcal G$ but nothing can be said about H

*** Q6
#+begin_src matlab
idGo = idpoly([],[0 0 0 0.10276 0.18123],[],[],[1 -1.99185 2.20265 -1.84083 0.89413])
#+end_src
** Part 2
*** Q1
Yes, it contains the same structure
*** Q2
Yes
*** Q3
No, $G\in\mathcal G$ but $H\in\mathcal H$
*** Q4
Yes, they confirm
*** Q5
#+begin_src matlab
idGo = idpoly([1 -1.99185 2.20265 -1.84083 0.89413],[0 0 0 0.10276 0.18123],[],[],[])
#+end_src
yes, same conclusions
* Computer Session 2 - 7 avril
** Questions
*** Q1
plot impulse unfiltered using =cra([y u],999,0,1)= se that after 600 there is no more increase in value, since correlation value is equal for all $\tau$
*** Q2
Using [4 4 4 4 1] cross correlation respects confidence bounds, so $G\in\mathcal G$ increase for H
*** Q5
Resonance peak near $0.5$ rad/s
*** Q6
[0 0.2] rad/s
use =(y-lsim(bj53551,u))=<0.4 \to not negligible
* Computer Session 3 - 7 avril
** Questions

* Closed-loop Identification
- lecturer  :: Paul Van der Hof

- [[https://en.wikipedia.org/wiki/Youla%E2%80%93Kucera_parametrization][Youla parametrization]]
** Direct Identification method
**** Data Informativity
is the data sufficiently informative so
**** Parsseval's relation
**** Relaxation of persistence excitation condition on r
if $r=0$ $\phi_z$ is rank deficient
**** Can be achieved
- persistently excitin u in open loop
- closed loop
  + presence of r
  + controller of sufficiently high order
  + time varying non linear controller
**** Summary
- consisten estimates if $S\in\mathcal M$
- excitation conditions can be realizea exciten r signal by excit
- no consis when only G\inmath g
- no free excitation of inuput
- unstable plants can be modeeled with arx armax
- *but noise modeles need to be accure estimated*
- in situatio consist, maxiumu likelihood resuls remain valid cramer rao lower bound
- result remain valid for non linea or time varyin controller
** Indirect identification methods
- Main step
  + additional use of r
  + use knowledge of C
  + utilizin the linearit of the closed loop system (linear controller)
- Several indirect methods
*** Coprime factor approach
- $y = G_0S_0r+S_0v$  \to G
  + estimate G_0S_0 is like an open loop estimation
- $u = S_or+CS_0v$
  + estimate S_0 is like an open loop estimation
- Estimate transfers $r \to \left[\begin{matrix}y\\u\end{matrix}\right]$
- Predictor model
- Estimate $\hat G={G_0S_0\over S_0}$
**** Consistency result
- $\hat G$ is a consistent estimate of $G_0$ if $H_{ind}(q,\eta)$ is parameterized independently from $\theta$
- N.B. we do not need to estimate a nois model to obtain a consistent estimate of $G_0$
*** Projection methors (two-stage / IV)
- decompose $u(t)=u^r(t)+u^e(t)$
- then $y(t)=G_0u^r(t)+G_0u^e(t)+H_0e(t)$
**** How to estimate $u^r(t)$?
- identify $G_{ur}$ (open-loop problem)
- Simulate $\hat u^r(t)=\hat G_{ur}(q)r(t)$
- use $\hat u^r(t)$ to identify $G_0$


*** Discussion
- consistency results of $G\in\mathcal G$
- effective use of $r$
- hard to prespecify the model order of estimate (quotient of two estimates)
*** Asymptotic variance
**** Direct method
+ variance results of the open-loop situation remain valid provided that have consistence $s \in mathhcal m$
+ includes maximum likelihood properties of the estimates mini variance asymptotically
+ both r and e contribute o variance reduction of \hat g unless n\to infty
+ asymptotic-in-order-of-G-and-H result for n $N\to \infty$
  $var\hat G(e^{i\omega}) \approx {n\over N}{ \phi_v(\omega) \over\phi_u^r(\omega)})$
**** indirect method
- typically reference r is used as input for identification
- typical variance result
  $var\hat G(e^{i\omega}) \approx {n\over N}{ \phi_v\omega \over\phi_u^r(\omega)})$
- only the reference-pare of the input signal contributes to variance reduction
- for finite model orders: neglecting u^e as input signal contributes to a worse SN-ratio
*** Model validation in cloed-loop
**** for all indirect methods:
validation with correlation tests as in open-loop
**** for direct method: careful with test on $R_{\epsilon u}(\tau)$
- past of e affects future of u
  $R_{ue}(\tau)\ne0\,\tau>0$
  $R_{eu}(\tau)\ne0\,\tau<0$
  if H(\þheta)=H_0 ok, if not, consistency fails
* Dynamics networks modeling
** Introduction
*** Dynamic networks
 - overall trend
   + interconnected and large scale
   + systmes of systems
   + distributed control modeling monitoring Optimization
   + Data (big data, machine learning, AI)

 - Learning models/actions form data

*** Distributed/multi-agent control
- both physical and communication links between systems $G_i$ and controllers $C_i$
*** Network models
**** State space representation
  - characteristcs
    - states as nodes in a directed graph
    - state transition (1 step in time) reflected by $a_{ij}$
    - transitions are encoded in *links*
    - effect of transitions are summed in the nodes
    - self loops are allowed
    - actuation ($u$) and sensing ($y$) reflected by separate links
  - For data-driven modeling problems \to Module representation
    + stronger role for measurable inputs and outputs
**** Module representation
  - modules ( TFs )
  - Vertices are node signals
  - basic building block
    $w_j(t)=\sum\limits_{i\in\mathcal N_j}G^0_{jk}(q)w_k(t)+r_j(t)+v_j(t)$
  - Collect all equations:
    - $w(t)=G^0(q)w(t)+R^0(q)r(t)+H^0(q)e(t)$
    - $w(t)=G^0(q)w(t)+R^0(q)r(t)+v(t)$
      $cov(e)=\Lambda$
    - G^0(q) is [[file:20200429185809-linear_algebra.org::*Hollow][hollow]]
  - Assumptions
    + total of L nodes no selfloops
    + network is well posed $(I-G^0)^{-1}$ exists
    + modules are dynamic LTI proper, may be unstable
    + disturbances canbe correlated: $H^0$ not necessarily diagonal
*** Example: Networks of (damped) oscillators
*** Example: Networks of (damped) oscillators
Gudi rawlings jouran ALchF Journal 52(6) doi:21982210.2008
*** Data-driven modeling
*** Model learning problems
- Under which condition can we estimate the topology and/org dynamics of the full network?
- How/when can we learn a local module from data (with known/unknown topology)? Which signals to measure?
- Where to optimally locate sensors and actuators, and design experiments?
- Same questions for subnetwork
- Can we benefit from a priori Known modules? (ex: controllers)
- Fault detection and diagnosis; detect/ handle nonlinear elements
- Can we distribute the computations?
*** Dynamic network setup - graph
- Mason
*** From SISO to MISO
MISO mappings may be non linear
*** Disturbance modeling - reduced rank noise
- if $dim(e)<dim(v)$: reduced rank
  - $H(z)$ is nonsquare \to problem with inversion for calculation of $\epsilon$
  - Spectral density $\Phi_v(z)=H(z)\Lambda H^T(z^{-1})$ is singular
*** Disturbance modeling - reduced rank noise
- unique spectral factorizations:

1. $\Phi_v(z)=H(z)\Lambda H^T(z^{-1})$
  a. $H(z):\mathbb{R}^{ L\times L}$,
  b. $\lambda :\mathbb{R}^{L\times L}$ singular
2. $\Phi_v(z)=\tilde H(z)\tilde \Lambda \tilde H^T(z^{-1})$
  a. $\tilde H(z):\mathbb{R}^{ L\times p}$,
  b. $\tilde \lambda :\mathbb{R}^{p\times p}$ regular
*** Summary
- several different approaches
- typical TF approach to include structure (topology)
- new data-driven challenges

* a
<2021-04-08 jeu. 14:00-14:35>
** Single module identification
- naive local approaches
  + based on w2 an w1
  + T_{w_2r_1}T^{-1}_{w_1r_1}
- do note work because of parallel paths
** Single module identification - full miso situation
- MISO ident prob
  + to be addressed by a closed-loop identification method (direct or indirect)
** Indirect methods
- MISO
  + select output $w_j$ and all its in-neighbors $w_\mathcal N$ as predictor output; $r_\mathcal D$ as predictor input
  + Estimate $\bar T_{\mathcal Nr}$ and $\bar T_{jr}$ consistently, and determine $\hat G_{j\mathcal N}=\hat T_{jr}\hat T^{-1}_{\mathcal Nr}$
  + or thorou IV or two stage method
  + freedo in location of r-signals
- Condition for consistency of \hat Gji
  + G^0\in G no noise requ
  + persint ex \phi rd > 0
  + ...
** Direct mehod
- Estimate transfer $w_\mathcal N\to w_j$ and model the disturbance process on the output
- provided the is enough excitation through external signals $r$ and $e$
- Conditions $\hat G_{ij}$
  + s\in Mathcal M
  + $\Phi_v$ is diagonal
  + every loop aroun wj has a delay
  + $\Phi_k(w) > 0$ with $k(t)= [w_j w_\mathcal N]^T$
** Summary
- all ocal nodes are measured
- closed-loop idenetication can be used
*** indirect
- noise models not required
- more expensiv exprimien
*** directhk
- noise models required
- minimuù varianc results
- requires dainoganl noise spectrum
*** both
- non convex algorithms are poorly scalable to large dimensions
** Empirical bayes method in local module identification
- MISO identification with all modules parameterized
- Brings 2 major problems
  + large number of parameters to estimate
  + model order selection step for each module CV AIC BIC
** Solution strategy
- divide models
- use gaussian process
** Empirical Bayes
- Parametric model  and (gaussian process +TC Kernel)

** Model parameters and hperparameters
- marginal pdf of wj p(wj;eta)
- depends on eta (has \lambdas and \betas as \sigma_j^2)

** algorithms for multi stage/sequential methods
*** Two staged method Empirical Bayes
*** model order reduction Steiglitz McBride (MORSM)
*** weighted null-space fitting (WNSF)
* Network Identifiability - Analysis
** Introduction - classical situations
-  models are indistinguishable from data if ther predictor filter are the same
** Introduction - classical situations
- if parameters are equal \to TFs are equal \to predictor filters equal \to predictor are equal
  - Identifiability of model structure
** Network Identifiability problem
- the network model can be transformed with any rational P(q) resulting to an equivalent model
- \to Nonuniqueness, unless there are structural constraints on G, R, H
** Network Identifiability problem
- Is there a unique map from $(T_{wr},\Phi_{\bar v})$ to $(G R H)$?
** Network Identifiability
- network defined by (G^0,R^0,H^0,\Lambda^0)
- network model defined by (G,R,H,\Lambda)
- network mode set by defined by $\mathcal M={M(\theta) =(G(\theta),R(\theta),H(\theta),\Lambda(\theta)),\theta\in\Theta}$
*** Definition
- For a network model set $\mathcal M$ consider a model $M(\theta_0)$ if $\forall M(\theta_1)$ the $T_{wr}(q,\theta_0)=T_{wr}(q,\theta_1)$ and $\Phi_{\bar v}(w,\theta_0)=\Phi_{\bar v}(w,\theta_1)$
** Network Identifiability
- under which conditions does $\Phi_{\bar v}=(I-G)^{-1}H\Lambda H^\star(I-G)^{-\star}$ provides a unique $T_{we}=(I-G)^-1H$
- Proposition
  1. modules in G are strictly proper
  2. no alebraic loops in G(\theta) and $H^\infty\Lambda {H^\infty}^T$
** First (conservative ) networked identifiability result
- full excitations case \to Q makes all nodes independently excited
** Second networked identifiability result
- number of parametrized in [G H R] < K+p
$\breve T$
** Generic identifiability
- Bazanella et al CDC 2017
** Generic rank
** Discussion identifiability
single property identifiability
** Summary
- determined by
  + topology
  + presence and location of external signals
  + presence and correlation structure of disturbances
- two different concepts:
  - global (algebraic)
  - generic (path-based conditions)
- Presented results all w assumed to be measurable
- fully applicable to situation p< L reduce rank noise
- full excitation case and general case
- results not yet suited for synthesis
* Network identifiability - Synthesis
** Network identifiability - Synthesis
** Synthesis
- question?
** Network identifiability - Synthesis
- definition Pseudotree
  - simple directed graph if for all vertices $i$ the number of neighbors is $\leq1$
** Network identifiability - Synthesis
- Strategy
  - cover the graph with set of disjoint pseudotrees
  - excite one of the roots of each pseudotree
** Network identifiability - Synthesis
- is identifiable if
  + K pseudotrees and K external signals noises
** Network identifiability - Synthesis
- set every node to be a pseudotree and then merge
** discussion identifiability - Synthesis
- node signals can not all be measured
  + separate problem | BazanellaEtAL CDC 2019
  + Situation analysed using concept of immersion | Dankers et al TAC 2016

* Single module identification
** Immersion
Full system identification
* Single module identification - local direct method
** Confounding variable:
- Confounding variable :: unmeasured signal that has (unmeasured paths) to both the input and output of an estimation problem.
***  algo
1. select input $w_i$ and out $w_j$
2. add inputs to satisfy the parallelr and loop condit
3. check on direct confound \to add output and return to setp2
4. check on indirect confounding variables
   a. add output an goto 2 or
   b. add input
* From sparsity-inducing estimation to hybrid system identification
<2021-04-09 ven. 08:30-09:34>
- lecturer :: Laurent Bako

** System identification
- inference of mathematical models from input-output
*** Basic principle of system [[file:20210323094314-identification.org][Identification]]
1. Run experiment collect (input output samples)
2. Fit data to chosen model (by [[file:20200709102805-optimization.org][Optimization]])
3. Check performance
*** Sparsity-inducing [[file:20200709102805-optimization.org][Optimization]]
- minimization of the cardinality o a set
  + Candès, Donoho etc
- Numerous applications in signal/image processing, information theory and machine learning...

*** Data-generating system
- $y_t=f^o(z_t)+v_t$
- $z_t$ is the regressor
- $f o$ is an unknown(possibly nonlinear) function
- $v_t$ represents model mismatches or measurement noise

*** Static/Dynamic
- static if z is unstructered multivariate
- dynamic if have structure \to (N-ARX) or (N-FIR)
*** Selection of a performance index
$V_N(\theta)={1 \over N}\sum_{t=1}^Nl(\epsilon_t(\theta))$
- $\theta^\star$ minimizes $V_N(\theta)$
*** Estimator design
- different functions for $l(\epsilon)$
**** Examples for $l$
+ if $v_t$ pertains to [[file:20200429185809-linear_algebra.org::*r-sparse][r-sparse]] I_0 binary
+ Quadratic loss $l(\epsilon)=\epsilon^2$
+ Absolute $l(\epsilon)=|\epsilon|$
 ...

 OBS: careful with large errors (try something "linear" for large errors like Vapnik or Huber)
*** Regression problem
- noise-free and moderate noise Least Squares can do the job well
  + but not in finite time
- Impulsive noise
  + Least Absolute deviation (LAD) can provide the exact parameter under sparsity assumption
  + performance does not depend on noise amplitude
** Sparsity-inducing robust estimation
<2021-04-09 ven. 09:47-10:52>
*** Robust estimation problem
$v_t=f_t+e_t$
- $e_t$ is a dense noise sequence
- $f_t$ is a sparse noise sequence
*** No dense noise
- $e_t=0$
- use of approximation of $l_0$ norm
*** When is the LAD estimator well-defined?
*** Some notations
- $\mathbb I^-$ negative error
- $\mathbb I^+$ positive error
- $\mathbb I^0$
*** Characterization of the solution set $\Psi(y,X)$
** Hybrid Systems
*** Example of switched system
- SEPIC DC-DC Converter
** Switched system identification
*** Direct approach
- algorithm for partition data
*** Generalized Principal Component Analysis
- VidalEtAl CDC2003


* Design of optimal identification experiments
<2021-04-09 ven. 14:06-15:00>
- lecturer :: [[file:xavier_bombois.org][Xavier Bombois]]
** Reminders and introduction
** Optimal experiment design
- robust control considerations $r_{adm}(\omega)$
- By choosing N or the power of $u(t)$ sufficiently large \to multiple solutions!
** Convexification of the optimization problem


* Videos
** 6 April 2021
- INTRODUCTION (slides_spring_1.pdf) :: https://replay.ec-lyon.fr/video/0782-2021-spring-school-linear-systemidentification-video-1-introduction/
- PREDICTION ERROR IDENTIFICATION (slides_spring_2.pdf) :: https://replay.ec-lyon.fr/video/0775-2021-spring-school-linear-systemidentification-video-2/
** 7 April 2021
- PREDICTION ERROR IDENTIFICATION (cont’d) (slides_spring_2.pdf) :: https://replay.ec-lyon.fr/video/0776-2021-spring-school-linear-systemidentification-video-3/
  https://replay.ec-lyon.fr/video/0777-2021-spring-school-linear-systemidentification-video-4/
  https://replay.ec-lyon.fr/video/0778-2021-spring-school-linear-systemidentification-video-5/
- ETFE (slides_spring_3.pdf) :: https://replay.ec-lyon.fr/video/0780-2021-spring-school-linear-systemidentification-video-6/
- EXPERIMENT DESIGN (slides_spring_4.pdf) :: https://replay.ec-lyon.fr/video/0781-2021-spring-school-linear-systemidentification-video-7/
- MATLAB TOOLBOX (presentation of the system identification toolbox of Matlab for the computer exercises) :: https://replay.ec-lyon.fr/video/0843-2021-spring-school-linear-systemidentification-toolbox/
- ADDITIONAL VIDEO :: Hereunder you will find an additional video corresponding to some slides in the file slides_spring_2.pdf (slides 143-161) that will not be covered on 7 April due to lack of time: https://replay.ec-lyon.fr/video/0779-2021-spring-school-linear-systemidentification-extra-video-pei/
** 8 April 2021

* Bibliography

** General books on System Identification:
- Lennart Ljung, /System Identification: theory for the user/ Prentice Hall, 1999
- Paul Van den Hof, /System Identification: data-driven modeling of dynamic systems/, 2020. Available at http://www.publications.pvandenhof.nl/5SMB0/ManuscrSysid_Febr2020.pdf
- Rik Pintelon and Johan Schoukens, /System Identification: a frequency-domain approach/, Wiley 2012

** A software for the design of optimal identification experiments (course of Friday afternoon) is available.
See https://hal.archives-ouvertes.fr/hal-03175027/document
